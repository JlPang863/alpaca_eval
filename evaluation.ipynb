{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlpang/alpaca_eval/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "eval_set = load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval\")['eval']\n",
    "\n",
    "\n",
    "# model_tag = \"pythia28_hh_selected_clean_subset\"  \n",
    "\n",
    "# model_name_or_path = f\"/mnt/data1/jinlong/compare_model_preferences/{model_tag}_model\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# def generate(instruction):\n",
    "#     inputs = tokenizer(instruction, return_tensors=\"pt\")    \n",
    "#     outputs = model.generate(**inputs, max_length=1024)  \n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# for example in tqdm(eval_set, desc=\"Generating AlpacaEval2's Repsonses\"):\n",
    "#     example['output'] = generate(example['instruction'])\n",
    "#     example['generator'] = model_tag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlpang/alpaca_eval/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a friendly chatbot who always responds in the style of a pirate<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How many helicopters can a human eat in one sitting?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"/mnt/data1/jinlong/DPO-noisy-outputs/llama-3-8b-kto\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)  # You may want to use bfloat16 and/or move to GPU here\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    " ]\n",
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "print(tokenizer.decode(tokenized_chat[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.62s/it]\n",
      "Generating AlpacaEval2's Responses:   0%|          | 0/51 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jlpang/fastChat/fastchat/llm_judge/alpacaeval2/AlpacaEval2_evaluation.ipynb 单元格 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.7/home/jlpang/fastChat/fastchat/llm_judge/alpacaeval2/AlpacaEval2_evaluation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(eval_set), batch_size), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGenerating AlpacaEval2\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms Responses\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.7/home/jlpang/fastChat/fastchat/llm_judge/alpacaeval2/AlpacaEval2_evaluation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     batch \u001b[39m=\u001b[39m eval_set[i:i\u001b[39m+\u001b[39mbatch_size]  \u001b[39m# 获取当前批次的数据\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.7/home/jlpang/fastChat/fastchat/llm_judge/alpacaeval2/AlpacaEval2_evaluation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     instructions \u001b[39m=\u001b[39m [example[\u001b[39m'\u001b[39m\u001b[39minstruction\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m batch]  \u001b[39m# 提取 instruction 列表\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.7/home/jlpang/fastChat/fastchat/llm_judge/alpacaeval2/AlpacaEval2_evaluation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     batch_outputs \u001b[39m=\u001b[39m generate_batch(instructions)  \u001b[39m# 获取当前批次的生成输出\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.7/home/jlpang/fastChat/fastchat/llm_judge/alpacaeval2/AlpacaEval2_evaluation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m# 将输出添加到结果列表\u001b[39;00m\n",
      "\u001b[1;32m/home/jlpang/fastChat/fastchat/llm_judge/alpacaeval2/AlpacaEval2_evaluation.ipynb 单元格 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.7/home/jlpang/fastChat/fastchat/llm_judge/alpacaeval2/AlpacaEval2_evaluation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(eval_set), batch_size), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGenerating AlpacaEval2\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms Responses\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.7/home/jlpang/fastChat/fastchat/llm_judge/alpacaeval2/AlpacaEval2_evaluation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     batch \u001b[39m=\u001b[39m eval_set[i:i\u001b[39m+\u001b[39mbatch_size]  \u001b[39m# 获取当前批次的数据\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.7/home/jlpang/fastChat/fastchat/llm_judge/alpacaeval2/AlpacaEval2_evaluation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     instructions \u001b[39m=\u001b[39m [example[\u001b[39m'\u001b[39;49m\u001b[39minstruction\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m batch]  \u001b[39m# 提取 instruction 列表\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.7/home/jlpang/fastChat/fastchat/llm_judge/alpacaeval2/AlpacaEval2_evaluation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     batch_outputs \u001b[39m=\u001b[39m generate_batch(instructions)  \u001b[39m# 获取当前批次的生成输出\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.7/home/jlpang/fastChat/fastchat/llm_judge/alpacaeval2/AlpacaEval2_evaluation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m# 将输出添加到结果列表\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\"\n",
    "\n",
    "# 加载评估数据集\n",
    "eval_set = load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval\")['eval']\n",
    "\n",
    "# 模型路径和标签\n",
    "model_tag = \"pythia28_hh_selected_clean_subset\"\n",
    "model_name_or_path = f\"/mnt/data1/jinlong/compare_model_preferences/{model_tag}_model\"\n",
    "\n",
    "# 加载模型和分词器\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# 设置批处理大小\n",
    "batch_size = 16\n",
    "\n",
    "def generate_batch(instructions):\n",
    "    # 将一批 instruction 编码成模型能理解的格式\n",
    "    inputs = tokenizer(instructions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # 将所有输入传递给模型并生成输出\n",
    "    outputs = model.generate(**inputs, max_length=1024)\n",
    "    \n",
    "    # 解码生成的输出并返回\n",
    "    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "# 按批次生成输出\n",
    "generated_outputs = []\n",
    "for i in tqdm(range(0, len(eval_set), batch_size), desc=\"Generating AlpacaEval2's Responses\"):\n",
    "    batch = eval_set[i:i+batch_size]  # 获取当前批次的数据\n",
    "    instructions = [example['instruction'] for example in batch]  # 提取 instruction 列表\n",
    "    batch_outputs = generate_batch(instructions)  # 获取当前批次的生成输出\n",
    "    \n",
    "    # 将输出添加到结果列表\n",
    "    for example, output in zip(batch, batch_outputs):\n",
    "        example['output'] = output\n",
    "        example['generator'] = model_tag\n",
    "        generated_outputs.append(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "# eval_set = load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval\")['eval']\n",
    "reference_outputs = load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval_gpt4_baseline\")['eval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What are the names of some famous actors that started their careers on Broadway?',\n",
       " 'output': 'Several famous actors started their careers on Broadway before making it big in film and television. Here are a few notable examples:\\n\\n1. Sarah Jessica Parker - Before she was Carrie Bradshaw on \"Sex and the City,\" Sarah Jessica Parker was a Broadway star, having appeared in productions like \"Annie\" as a child.\\n\\n2. Meryl Streep - Meryl Streep\\'s early career included Broadway productions such as \"Trelawny of the \\'Wells\\'\" and \"A Memory of Two Mondays / 27 Wagons Full of Cotton.\"\\n\\n3. Hugh Jackman - Hugh Jackman won a Tony Award for his role in \"The Boy from Oz\" and has been known for his stage work as well as his film career.\\n\\n4. Sutton Foster - Known for her television role in \"Younger,\" Sutton Foster is also a Broadway legend with leading roles in shows like \"Thoroughly Modern Millie\" and \"Anything Goes.\"\\n\\n5. Kristen Bell - Before she was the voice of Anna in \"Frozen\" or the star of \"The Good Place,\" Kristen Bell appeared in Broadway\\'s \"The Adventures of Tom Sawyer\" and \"The Crucible.\"\\n\\n6. Audra McDonald - Audra McDonald is a renowned Broadway actress with a record-breaking number of Tony Awards. She\\'s starred in \"Ragtime,\" \"Carousel,\" \"Master Class,\" and more.\\n\\n7. Nathan Lane - Nathan Lane is a Broadway veteran known for his roles in \"The Producers,\" \"A Funny Thing Happened on the Way to the Forum,\" and \"Angels in America.\"\\n\\n8. Idina Menzel - Before \"Frozen\" and \"Wicked\" made her a household name, Idina Menzel started on Broadway in shows like \"Rent\" and \"Hair.\"\\n\\n9. Lin-Manuel Miranda - Before \"Hamilton\" and \"In the Heights\" became huge hits, Lin-Manuel Miranda was performing on Broadway, eventually becoming a celebrated writer and actor.\\n\\n10. Lea Michele - Prior to her role on \"Glee,\" Lea Michele was a young Broadway actress in shows like \"Les Misérables,\" \"Ragtime,\" and \"Spring Awakening.\"\\n\\nThese actors are just a few examples of the many performers who have transitioned from the Broadway stage to broader fame in the entertainment industry. Broadway often serves as a proving ground for talent, and many actors continue to return to the stage throughout their careers.',\n",
       " 'generator': 'gpt4_1106_preview',\n",
       " 'dataset': 'helpful_base'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output', 'generator', 'dataset'],\n",
       "    num_rows: 805\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>win_rate</th>\n",
       "      <th>standard_error</th>\n",
       "      <th>mode</th>\n",
       "      <th>avg_length</th>\n",
       "      <th>n_wins</th>\n",
       "      <th>n_wins_base</th>\n",
       "      <th>n_draws</th>\n",
       "      <th>n_total</th>\n",
       "      <th>discrete_win_rate</th>\n",
       "      <th>length_controlled_winrate</th>\n",
       "      <th>lc_standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-3-8b-ours1-1-merged</td>\n",
       "      <td>42.105263</td>\n",
       "      <td>11.63728</td>\n",
       "      <td>community</td>\n",
       "      <td>3441</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>42.105263</td>\n",
       "      <td>39.924597</td>\n",
       "      <td>2.802031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Unnamed: 0   win_rate  standard_error       mode  \\\n",
       "0  llama-3-8b-ours1-1-merged  42.105263        11.63728  community   \n",
       "\n",
       "   avg_length  n_wins  n_wins_base  n_draws  n_total  discrete_win_rate  \\\n",
       "0        3441       8           11        0       19          42.105263   \n",
       "\n",
       "   length_controlled_winrate  lc_standard_error  \n",
       "0                  39.924597           2.802031  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_json('./results/llama-3-1b-dpo/model_outputs.json')\n",
    "\n",
    "# loss_type = \"dpo-new\"\n",
    "loss_type = \"ours1-1\"\n",
    "leaderboard = pd.read_csv(f\"model_outputs/llama-3-8b-{loss_type}-merged/alpaca_eval_gpt4/leaderboard.csv\")\n",
    "leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"model_outputs/llama-3-8b-ours4-1-new1/model_outputs_full.json\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "\n",
    "count = 0\n",
    "for output in dataset['output']:\n",
    "    if output == '':\n",
    "        count+=1\n",
    "    lens.append(len(output))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eval_set = load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval\")['eval']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlpang/alpaca_eval/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards: 100%|██████████| 4/4 [05:39<00:00, 84.77s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]\n",
      "Downloading shards: 100%|██████████| 4/4 [04:57<00:00, 74.34s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.57it/s]\n",
      "Downloading shards: 100%|██████████| 4/4 [05:24<00:00, 81.12s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "loss_types = [\"SimPO\", \"KTO\", 'IPO'] # 'CPO',  'DPO'\n",
    "\n",
    "for loss_type in loss_types:\n",
    "\n",
    "    model_name_or_path = f\"princeton-nlp/Llama-3-Base-8B-SFT-{loss_type}\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 3/3 [02:18<00:00, 46.21s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.21it/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [05:02<00:00, 100.97s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.35it/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [05:06<00:00, 102.20s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [04:52<00:00, 97.50s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.28it/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [05:15<00:00, 105.19s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "\n",
    "loss_types = [\"SimPO\", \"KTO\", 'IPO', 'CPO',  'DPO'] # \n",
    "\n",
    "for loss_type in loss_types:\n",
    "\n",
    "    model_name_or_path = f\"princeton-nlp/Mistral-7B-Base-SFT-{loss_type}\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "    tokenizer =AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.64it/s]\n",
      "model-00002-of-00007.safetensors: 100%|██████████| 4.83G/4.83G [02:33<00:00, 31.5MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.2M/17.2M [00:00<00:00, 30.8MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jlpang888/Llama-3-Base-8B-SFT-SimPO/commit/b666b67c788f90a10029cb8ff5af12a78d3faf89', commit_message='Upload tokenizer', commit_description='', oid='b666b67c788f90a10029cb8ff5af12a78d3faf89', pr_url=None, repo_url=RepoUrl('https://huggingface.co/jlpang888/Llama-3-Base-8B-SFT-SimPO', endpoint='https://huggingface.co', repo_type='model', repo_id='jlpang888/Llama-3-Base-8B-SFT-SimPO'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"princeton-nlp/Llama-3-Base-8B-SFT-SimPO\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/Llama-3-Base-8B-SFT-CPO\")\n",
    "\n",
    "model.push_to_hub(\"jlpang888/Llama-3-Base-8B-SFT-SimPO\")\n",
    "tokenizer.push_to_hub(\"jlpang888/Llama-3-Base-8B-SFT-SimPO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/3 [01:14<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jlpang/alpaca_eval/evaluation.ipynb 单元格 14\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.7/home/jlpang/alpaca_eval/evaluation.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.7/home/jlpang/alpaca_eval/evaluation.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mprinceton-nlp/Mistral-7B-Base-SFT-SLiC-HF\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.7/home/jlpang/alpaca_eval/evaluation.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.7/home/jlpang/alpaca_eval/evaluation.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[0;32m~/alpaca_eval/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    566\u001b[0m     )\n\u001b[1;32m    567\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/alpaca_eval/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3990\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3987\u001b[0m \u001b[39m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   3988\u001b[0m \u001b[39mif\u001b[39;00m is_sharded:\n\u001b[1;32m   3989\u001b[0m     \u001b[39m# resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 3990\u001b[0m     resolved_archive_file, sharded_metadata \u001b[39m=\u001b[39m get_checkpoint_shard_files(\n\u001b[1;32m   3991\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3992\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3993\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   3994\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   3995\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   3996\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   3997\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   3998\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   3999\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   4000\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   4001\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   4002\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   4003\u001b[0m     )\n\u001b[1;32m   4005\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   4006\u001b[0m     is_safetensors_available()\n\u001b[1;32m   4007\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(resolved_archive_file, \u001b[39mstr\u001b[39m)\n\u001b[1;32m   4008\u001b[0m     \u001b[39mand\u001b[39;00m resolved_archive_file\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.safetensors\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   4009\u001b[0m ):\n\u001b[1;32m   4010\u001b[0m     \u001b[39mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/alpaca_eval/venv/lib/python3.10/site-packages/transformers/utils/hub.py:1098\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[39mfor\u001b[39;00m shard_filename \u001b[39min\u001b[39;00m tqdm(shard_filenames, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading shards\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1096\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m         \u001b[39m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m         cached_filename \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m   1099\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m   1100\u001b[0m             shard_filename,\n\u001b[1;32m   1101\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1102\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1103\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1104\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   1105\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1106\u001b[0m             token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1107\u001b[0m             user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   1108\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1109\u001b[0m             subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   1110\u001b[0m             _commit_hash\u001b[39m=\u001b[39;49m_commit_hash,\n\u001b[1;32m   1111\u001b[0m         )\n\u001b[1;32m   1112\u001b[0m     \u001b[39m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m     \u001b[39m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[39mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m~/alpaca_eval/venv/lib/python3.10/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    404\u001b[0m         path_or_repo_id,\n\u001b[1;32m    405\u001b[0m         filename,\n\u001b[1;32m    406\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    407\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    408\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    409\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    410\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    411\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    412\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    413\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    414\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    415\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[39m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/alpaca_eval/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/alpaca_eval/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[39mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    841\u001b[0m         \u001b[39m# Destination\u001b[39;00m\n\u001b[1;32m    842\u001b[0m         local_dir\u001b[39m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m         local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[1;32m    858\u001b[0m     )\n\u001b[1;32m    859\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     \u001b[39mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[1;32m    861\u001b[0m         \u001b[39m# Destination\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    863\u001b[0m         \u001b[39m# File info\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m         repo_id\u001b[39m=\u001b[39;49mrepo_id,\n\u001b[1;32m    865\u001b[0m         filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m    866\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    867\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    868\u001b[0m         \u001b[39m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    869\u001b[0m         endpoint\u001b[39m=\u001b[39;49mendpoint,\n\u001b[1;32m    870\u001b[0m         etag_timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m    871\u001b[0m         headers\u001b[39m=\u001b[39;49mhf_headers,\n\u001b[1;32m    872\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    873\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    874\u001b[0m         \u001b[39m# Additional options\u001b[39;49;00m\n\u001b[1;32m    875\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    876\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    877\u001b[0m     )\n",
      "File \u001b[0;32m~/alpaca_eval/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1009\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1007\u001b[0m Path(lock_path)\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mmkdir(parents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1008\u001b[0m \u001b[39mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1009\u001b[0m     _download_to_tmp_and_move(\n\u001b[1;32m   1010\u001b[0m         incomplete_path\u001b[39m=\u001b[39;49mPath(blob_path \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.incomplete\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1011\u001b[0m         destination_path\u001b[39m=\u001b[39;49mPath(blob_path),\n\u001b[1;32m   1012\u001b[0m         url_to_download\u001b[39m=\u001b[39;49murl_to_download,\n\u001b[1;32m   1013\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1014\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1015\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[1;32m   1016\u001b[0m         filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m   1017\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1019\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1020\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/alpaca_eval/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1543\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[39m.\u001b[39mparent)\n\u001b[1;32m   1541\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[39m.\u001b[39mparent)\n\u001b[0;32m-> 1543\u001b[0m     http_get(\n\u001b[1;32m   1544\u001b[0m         url_to_download,\n\u001b[1;32m   1545\u001b[0m         f,\n\u001b[1;32m   1546\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1547\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m   1548\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1549\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[1;32m   1550\u001b[0m     )\n\u001b[1;32m   1552\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownload complete. Moving file to \u001b[39m\u001b[39m{\u001b[39;00mdestination_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1553\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/alpaca_eval/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:452\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    450\u001b[0m new_resume_size \u001b[39m=\u001b[39m resume_size\n\u001b[1;32m    451\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39mconstants\u001b[39m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    453\u001b[0m         \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    454\u001b[0m             progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/alpaca_eval/venv/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/alpaca_eval/venv/lib/python3.10/site-packages/urllib3/response.py:1066\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1066\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m   1068\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m   1069\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/alpaca_eval/venv/lib/python3.10/site-packages/urllib3/response.py:955\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    952\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m amt:\n\u001b[1;32m    953\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[0;32m--> 955\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[1;32m    957\u001b[0m flush_decoder \u001b[39m=\u001b[39m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m (amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data)\n\u001b[1;32m    959\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/alpaca_eval/venv/lib/python3.10/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    876\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 879\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt, read1\u001b[39m=\u001b[39;49mread1) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    880\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    881\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    888\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/alpaca_eval/venv/lib/python3.10/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread1(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread1()\n\u001b[1;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    861\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[1;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1304\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1160\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_name=\"princeton-nlp/Llama-3-Base-8B-SFT-SLiC-HF\"\n",
    "model_name = \"princeton-nlp/Mistral-7B-Base-SFT-SLiC-HF\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlpang/alpaca_eval/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of responses: 805\n",
      "Avg response length (chars): 7761.79\n",
      "Avg response length (tokens): 1620.59\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Llama-3-Base-8B-SFT-SimPO\n",
    "model_name_or_path = \"Llama-3-Base-8B-SFT-SimPO\"\n",
    "json_path = \"model_outputs_cl/Llama-3-Base-8B-SFT-SimPO/model_outputs_full.json\"\n",
    "\n",
    "# 修改为你的 model 路径\n",
    "# model_name_or_path = \"/mnt/data1/jinlong/CL_DPO_outputs/llama-3-8b-dpo-sorted-llama-full\"\n",
    "# json_path = \"./model_outputs_cl/llama-3-8b-dpo-sorted-llama-full-original/model_outputs_full.json\"\n",
    "\n",
    "# 加载 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# 加载生成结果\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 获取 response 列表\n",
    "responses = data['output']\n",
    "\n",
    "# 字符长度统计\n",
    "char_lengths = [len(r) for r in responses]\n",
    "\n",
    "# token 数统计\n",
    "token_lengths = [len(tokenizer.encode(r, add_special_tokens=False)) for r in responses]\n",
    "\n",
    "print(f\"Number of responses: {len(responses)}\")\n",
    "print(f\"Avg response length (chars): {np.mean(char_lengths):.2f}\")\n",
    "print(f\"Avg response length (tokens): {np.mean(token_lengths):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 模型名称，例如 llama 或 mistral\n",
    "model_name = \"jlpang888/llama-3-8b-ultrafeedback-sft-chosen\"\n",
    "\n",
    "# 本地保存路径（不存在会自动创建）\n",
    "save_directory = \"/mnt/data1/jinlong/CL_DPO_outputs/llama-3-8b-base-sft-chosen\"\n",
    "\n",
    "# 加载模型和 tokenizer（会从 Hugging Face 下载）\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 保存到本地路径\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
